# lightning.pytorch==2.1.3
seed_everything: false
eval_after_fit: true
trainer:
  accelerator: gpu
  devices: 1
  precision: 16-mixed
  max_epochs: 200
  logger:
    class_path: lightning.pytorch.loggers.TensorBoardLogger
    init_args:
      save_dir: logs/wideresnet28x10
      name: mixup_ensemble
      default_hp_metric: false
  callbacks:
    - class_path: torch_uncertainty.callbacks.TUClsCheckpoint
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val/cls/Acc
        patience: 1000
        check_finite: true
routine:
  model:
    class_path: torch_uncertainty.models.deep_ensembles
    init_args:
      core_models:
        class_path: torch_uncertainty.models.classification.wideresnet28x10
        init_args:
          in_channels: 3
          num_classes: 10
          style: cifar
          dropout_rate: 0.0
      num_estimators: 4
      task: classification
      # replace by yours
      # torch-uncertainty/experiments/classification/cifar10/logs/wideresnet28x10/mixup/version_0/checkpoints/epoch=194-step=76245-val_nll=0.222.ckpt,
      # torch-uncertainty/experiments/classification/cifar10/logs/wideresnet28x10/mixup/version_1/checkpoints/epoch=188-step=73899-val_nll=0.218.ckpt,
      # torch-uncertainty/experiments/classification/cifar10/logs/wideresnet28x10/mixup/version_2/checkpoints/epoch=183-step=71944-val_nll=0.211.ckpt,
      # torch-uncertainty/experiments/classification/cifar10/logs/wideresnet28x10/mixup/version_3/checkpoints/epoch=197-step=77418-val_nll=0.216.ckpt,
      ckpt_paths: []
  num_classes: 10
  is_ensemble: true
  format_batch_fn:
    class_path: torch_uncertainty.transforms.RepeatTarget
    init_args:
      num_repeats: 4
data:
  root: ./data
  batch_size: 128
